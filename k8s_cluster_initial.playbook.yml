# TCP	Inbound	6443	Kubernetes API server	All
# TCP	Inbound	2379-2380	etcd server client API	kube-apiserver, etcd
# TCP	Inbound	10250	Kubelet API	Self, Control plane
# TCP	Inbound	10259	kube-scheduler	Self
# TCP	Inbound	10257	kube-controller-manager	Self

# TCP	Inbound	10250	Kubelet API	Self, Control plane
# TCP	Inbound	30000-32767	NodePort Services†	All

- name: Initialize the first control plane
  hosts: master_1
  vars:
    # 10.128.0.0/10: 10.128.0.0 -> 10.191.255.255, 4.194.304 pod ips. 16384 nodes.
    # Apply Dual-stack support.
    # Calculate network size by https://www.ipaddressguide.com/ipv6-cidr
    pod_network_cidr_v4: 10.128.0.0/10
    cilium_pod_network_cidr_v4: "{{ pod_network_cidr_v4 }}"
    # 2001:db8:42:0::/56: 2001:0DB8:0042:0000:0000:0000:0000:0000 -> 2001:0DB8:0042:00FF:FFFF:FFFF:FFFF:FFFF
    # 4.722.366.482.869.645.213.696 pod ips.
    pod_network_cidr_v6: 2001:db8:42:0::/56
    # 2001:db8:42:0::/106: 2001:0DB8:0042:0000:0000:0000:0000:0000 -> 2001:0DB8:0042:0000:0000:0000:003F:FFFF, 4.194.304 pod ips. 16384 nodes.
    cilium_pod_network_cidr_v6: 2001:db8:42:0::/106
    # 10.16.0.0/12: 10.16.0.0 -> 10.31.255.255, 1.048.576 services
    service_cidr_v4: 10.16.0.0/12
    # 2001:db8:42:1::/108: 2001:0DB8:0042:0001:0000:0000:0000:0000 -> 2001:0DB8:0042:0001:0000:0000:000F:FFFF, 1.048.576 services
    service_cidr_v6: 2001:db8:42:1::/112
    cluster_domain: cluster.internal # Default: cluster.local
  tasks:
    - name: Check kubelet info for sure the current node is not in any cluster
      ansible.builtin.stat:
        path: "/etc/kubernetes/kubelet.conf"
      register: kubernetes_kubelet_conf

    - name: Check if apiserver port is listening
      ansible.builtin.wait_for:
        host: 0.0.0.0
        port: "{{ apiserver_bind_port }}"
        timeout: 1
        msg: "Timeout waiting for port {{ apiserver_bind_port }} to respond"
      register: apiserverportcheck
      ignore_errors: true

    - name: Initialize the cluster
      become: true
      ansible.builtin.command:
        argv:
          - kubeadm
          - init
          - --control-plane-endpoint={{ hostvars.master_lb_1.private_ip }}:{{ control_plane_endpoint_port }}
          - --apiserver-bind-port={{ apiserver_bind_port }}
          - --cri-socket=unix:///var/run/containerd/containerd.sock
          - --service-dns-domain={{ cluster_domain }}
          - --upload-certs
          - --apiserver-cert-extra-sans={{ hostvars.master_lb_1.ansible_host }}
          # If you want to use Cilium’s kube-proxy replacement, kubeadm needs to skip the kube-proxy deployment phase
          # https://docs.cilium.io/en/stable/installation/k8s-install-kubeadm/#create-the-cluster
          - --skip-phases=addon/kube-proxy
          # If we have never issued in the initialize state, we can manually do following
          # remove current apiserver certificates
          # sudo rm /etc/kubernetes/pki/apiserver.*
          # generate new certificates
          # sudo kubeadm init phase certs apiserver --apiserver-cert-extra-sans=localhost,127.0.0.1,54.179.193.197
          ##############################################################################
          # Network
          # We should design the CIDR range that suites with our need.
          # It will be better when we control multiple clusters and want to connect each other
          # The setting below is just maximized the size of network
          - --pod-network-cidr={{ pod_network_cidr_v4 }},{{ pod_network_cidr_v6 }}
          - --service-cidr={{ service_cidr_v4 }},{{ service_cidr_v6 }}
        chdir: $HOME
      register: initial_cluster_output
      when:
        - apiserverportcheck.failed
        - not kubernetes_kubelet_conf.stat.exists
      failed_when: initial_cluster_output.rc != 0
      changed_when: true

    - name: Get stat of /etc/kubernetes/admin.conf
      ansible.builtin.stat:
        path: "/etc/kubernetes/admin.conf"
      register: kubernetes_admin_conf

    - name: Mkdir .kube directory
      ansible.builtin.file:
        path: "$HOME/.kube"
        state: "directory"
        mode: "0755"

    - name: Copies admin.conf to user's kube config
      become: true
      # become_user: "{{ hostvars[inventory_hostname].ansible_user }}"
      ansible.builtin.copy:
        src: /etc/kubernetes/admin.conf
        dest: "{{ ansible_env.HOME }}/.kube/config"
        mode: a+r
        remote_src: true
      when: kubernetes_admin_conf.stat.exists

    # helm repo add cilium https://helm.cilium.io/
    - name: Add cilium chart repo
      kubernetes.core.helm_repository:
        name: cilium
        repo_url: "https://helm.cilium.io/"

    # https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/#kubeproxy-free
    # helm install cilium cilium/cilium --version 1.14.0 --namespace kube-system
    # https://docs.cilium.io/en/v1.14/helm-reference/
    - name: Deploy cilium
      kubernetes.core.helm:
        name: cilium
        chart_ref: cilium/cilium
        chart_version: 1.14.0
        release_namespace: cilium-network
        create_namespace: true
        set_values:
          # Replace kube-proxy
          - value: kubeProxyReplacement=true
          # kubernetes API address
          # Do not use kubernetes API address by ClusterIP because kube-proxy is gone this time
          - value: k8sServiceHost={{ hostvars.master_lb_1.private_ip }}
          - value: k8sServicePort={{ control_plane_endpoint_port }}
          # Do so by providing new values to Helm and applying them to the existing installation.
          # The agent pods will be gradually restarted.
          - value: rollOutCiliumPods=true
          - value: ipv4.enabled=true
          - value: ipv6.enabled=true
          - value: ipam.mode=cluster-pool
          - value: ipam.operator.clusterPoolIPv4PodCIDRList={{ cilium_pod_network_cidr_v4 }}
          - value: ipam.operator.clusterPoolIPv6PodCIDRList={{ cilium_pod_network_cidr_v6 }}
          - value: ipam.operator.clusterPoolIPv4MaskSize=24
          - value: ipam.operator.clusterPoolIPv6MaskSize=112
          - value: hubble.relay.enabled=true
          - value: hubble.peerService.clusterDomain={{ cluster_domain }}
          - value: hubble.ui.enabled=true

    # helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
    - name: Add metrics-server chart repo
      kubernetes.core.helm_repository:
        name: metrics-server
        repo_url: "https://kubernetes-sigs.github.io/metrics-server/"

    # helm install my-metrics-server metrics-server/metrics-server --version 3.11.0
    - name: Deploy metrics-server
      kubernetes.core.helm:
        name: metrics-server
        chart_ref: metrics-server/metrics-server
        chart_version: 3.11.0
        release_namespace: metrics-server
        create_namespace: true
        set_values:
          # Do not verify the CA of serving certificates presented by Kubelets. For testing purposes only.
          # Refer --kubelet-insecure-tls on https://github.com/kubernetes-sigs/metrics-server#configuration
          # Refer args on https://artifacthub.io/packages/helm/metrics-server/metrics-server
          # TODO: Bootstrap kubelet TLS https://kubernetes.io/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/#client-and-serving-certificates
          - value: args[0]=--kubelet-insecure-tls

    - name: Copy kubeconfig to localhost
      become: true
      ansible.builtin.fetch:
        src: /etc/kubernetes/admin.conf
        dest: kubeconfig
        flat: true

    - name: Replace private ip to public ip in kubeconfig
      ansible.builtin.replace:
        path: kubeconfig
        regexp: "{{ hostvars.master_lb_1.private_ip }}"
        replace: "{{ hostvars.master_lb_1.ansible_host }}"
      delegate_to: localhost
