# TCP	Inbound	6443	Kubernetes API server	All
# TCP	Inbound	2379-2380	etcd server client API	kube-apiserver, etcd
# TCP	Inbound	10250	Kubelet API	Self, Control plane
# TCP	Inbound	10259	kube-scheduler	Self
# TCP	Inbound	10257	kube-controller-manager	Self

# TCP	Inbound	10250	Kubelet API	Self, Control plane
# TCP	Inbound	30000-32767	NodePort Services†	All

- name: Initialize the first control plane
  hosts: master_1
  tasks:
    - name: Check kubelet info for sure the current node is not in any cluster
      ansible.builtin.stat:
        path: "/etc/kubernetes/kubelet.conf"
      register: kubernetes_kubelet_conf

    - name: Check if apiserver port is listening
      ansible.builtin.wait_for:
        host: 0.0.0.0
        port: "{{ apiserver_bind_port }}"
        timeout: 1
        msg: "Timeout waiting for port {{ apiserver_bind_port }} to respond"
      register: apiserverportcheck
      ignore_errors: true

    - name: Initialize the cluster
      become: true
      ansible.builtin.command:
        argv:
          - kubeadm
          - init
          - --control-plane-endpoint={{ hostvars.master_lb_1.private_ip }}:{{ control_plane_endpoint_port }}
          - --apiserver-bind-port={{ apiserver_bind_port }}
          - --cri-socket=unix:///var/run/containerd/containerd.sock
          - --service-dns-domain={{ cluster_domain }}
          - --upload-certs
          - --apiserver-cert-extra-sans={{ hostvars.master_lb_1.ansible_host }}
          # If you want to use Cilium’s kube-proxy replacement, kubeadm needs to skip the kube-proxy deployment phase
          # https://docs.cilium.io/en/stable/installation/k8s-install-kubeadm/#create-the-cluster
          - --skip-phases=addon/kube-proxy
          # If we have never issued in the initialize state, we can manually do following
          # remove current apiserver certificates
          # sudo rm /etc/kubernetes/pki/apiserver.*
          # generate new certificates
          # sudo kubeadm init phase certs apiserver --apiserver-cert-extra-sans=localhost,127.0.0.1,54.179.193.197
          ##############################################################################
          # Network
          # We should design the CIDR range that suites with our need.
          # It will be better when we control multiple clusters and want to connect each other
          # The setting below is just maximized the size of network
          - --pod-network-cidr={{ pod_network_cidr_v4 }},{{ pod_network_cidr_v6 }}
          - --service-cidr={{ service_cidr_v4 }},{{ service_cidr_v6 }}
        chdir: $HOME
      register: initial_cluster_output
      when:
        - apiserverportcheck.failed
        - not kubernetes_kubelet_conf.stat.exists
      failed_when: initial_cluster_output.rc != 0
      changed_when: true

    - name: Get stat of /etc/kubernetes/admin.conf
      ansible.builtin.stat:
        path: "/etc/kubernetes/admin.conf"
      register: kubernetes_admin_conf

    - name: Mkdir .kube directory
      ansible.builtin.file:
        path: "$HOME/.kube"
        state: "directory"
        mode: "0755"

    - name: Copies admin.conf to user's kube config
      become: true
      # become_user: "{{ hostvars[inventory_hostname].ansible_user }}"
      ansible.builtin.copy:
        src: /etc/kubernetes/admin.conf
        dest: "{{ ansible_env.HOME }}/.kube/config"
        mode: a+r
        remote_src: true
      when: kubernetes_admin_conf.stat.exists

    - name: Copy kubeconfig to localhost
      become: true
      ansible.builtin.fetch:
        src: /etc/kubernetes/admin.conf
        dest: kubeconfig
        flat: true

    - name: Replace private ip to public ip in kubeconfig
      ansible.builtin.replace:
        path: kubeconfig
        regexp: "{{ hostvars.master_lb_1.private_ip }}"
        replace: "{{ hostvars.master_lb_1.ansible_host }}"
      delegate_to: localhost
